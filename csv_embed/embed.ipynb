{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5854cef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Torch device available: cuda, CUDA count=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ff4f4c661c4583bcfccfc9d3db5d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded on device(s): {device(type='cuda', index=0), device(type='meta')}\n",
      "\n",
      "üìÑ Loading CSV: tech_law_violations.csv\n",
      "\n",
      "üöÄ Ingesting one doc: 0\n",
      "Split into 2 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9879\n",
      "‚û°Ô∏è Embedded batch 2 on cuda:0 | shape=(1, 4000) | norm‚âà0.9924\n",
      "‚úÖ Upserted 2 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 1\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9916\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 2\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9933\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 3\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9951\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 4\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9917\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 5\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9889\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 6\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9881\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 7\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9911\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 8\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9893\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 9\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9891\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "‚Ä¶ processed 10/51 rows\n",
      "\n",
      "üöÄ Ingesting one doc: 10\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9869\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 11\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9899\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 12\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9900\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 13\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9918\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 14\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9915\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 15\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9910\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 16\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9856\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 17\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9877\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 18\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9884\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 19\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9887\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "‚Ä¶ processed 20/51 rows\n",
      "\n",
      "üöÄ Ingesting one doc: 20\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9858\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 21\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9952\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 22\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9918\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 23\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9923\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 24\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9945\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 25\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9908\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 26\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9918\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 27\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9945\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 28\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9862\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 29\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9914\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "‚Ä¶ processed 30/51 rows\n",
      "\n",
      "üöÄ Ingesting one doc: 30\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9930\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 31\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9930\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 32\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9934\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 33\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9904\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 34\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9892\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 35\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9898\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 36\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9903\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 37\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9902\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 38\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9901\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 39\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9930\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "‚Ä¶ processed 40/51 rows\n",
      "\n",
      "üöÄ Ingesting one doc: 40\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9909\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 41\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9906\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 42\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9936\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 43\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9950\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 44\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9952\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 45\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9902\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 46\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9862\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 47\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9868\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 48\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9902\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: 49\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9911\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "‚Ä¶ processed 50/51 rows\n",
      "\n",
      "üöÄ Ingesting one doc: 50\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9904\n",
      "‚úÖ Upserted 1 rows into Supabase.\n"
     ]
    }
   ],
   "source": [
    "# embed.py\n",
    "# pip install pandas transformers \"supabase>=2\" python-dotenv torch\n",
    "\n",
    "import os, re, json, hashlib\n",
    "from typing import List, Dict, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from supabase import create_client, Client\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---- Env ----\n",
    "load_dotenv(\"../secrets/.env.dev\")\n",
    "SUPABASE_URL = os.environ[\"SUPABASE_URL\"]\n",
    "SUPABASE_KEY = os.environ[\"SUPABASE_KEY\"]\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# ---- Model ----\n",
    "MODEL_ID = \"Qwen/Qwen3-Embedding-8B\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üîß Torch device available: {DEVICE}, CUDA count={torch.cuda.device_count()}\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n",
    "mdl = AutoModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.bfloat16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",        # will use GPU if possible\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "mdl.eval()\n",
    "print(f\"‚úÖ Model loaded on device(s): {set(p.device for p in mdl.parameters())}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def _last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    seq_lens = attention_mask.sum(dim=1) - 1\n",
    "    bsz = last_hidden_states.shape[0]\n",
    "    return last_hidden_states[torch.arange(bsz, device=last_hidden_states.device), seq_lens]\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_texts(texts: List[str], batch_size=1, max_length=1024) -> np.ndarray:\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        toks = tok(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        dev = next(iter(mdl.state_dict().values())).device\n",
    "        toks = {k: v.to(dev) for k, v in toks.items()}\n",
    "        out = mdl(**toks)\n",
    "        pooled = _last_token_pool(out.last_hidden_state, toks[\"attention_mask\"])\n",
    "        pooled = F.normalize(pooled, p=2, dim=1)\n",
    "        pooled = pooled[:, :4000].to(torch.float32)  # keep 4000 for halfvec(4000)\n",
    "        arr = pooled.cpu().numpy()\n",
    "        norms = np.linalg.norm(arr, axis=1)\n",
    "        print(f\"‚û°Ô∏è Embedded batch {i//batch_size+1} on {dev} | shape={arr.shape} | norm‚âà{norms.mean():.4f}\")\n",
    "        vecs.append(arr)\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "# =========================\n",
    "# NEWS-AWARE CHUNKER\n",
    "# =========================\n",
    "BOILERPLATE_PATTERNS = [\n",
    "    r\"^Advertisement$\", r\"^Ads?$\", r\"^Subscribe now.*$\", r\"^Sign up for.*newsletter.*$\",\n",
    "    r\"^Read more:.*$\", r\"^Editor‚Äôs note:.*$\", r\"^Correction:.*$\", r\"^¬©.*$\"\n",
    "]\n",
    "BOILERPLATE_RE = re.compile(\"|\".join(BOILERPLATE_PATTERNS), flags=re.IGNORECASE)\n",
    "\n",
    "def _clean_lines(text: str) -> List[str]:\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\")]\n",
    "    return [ln for ln in lines if ln and not BOILERPLATE_RE.match(ln)]\n",
    "\n",
    "def _looks_headline(s: str) -> bool:\n",
    "    return len(s) <= 140 and not s.endswith(\".\") and (\n",
    "        s.istitle() or s.isupper() or re.match(r\"^[A-Z0-9][A-Za-z0-9,:‚Äô'‚Äú‚Äù\\-‚Äì ]+$\", s)\n",
    "    )\n",
    "\n",
    "def _is_dek_or_byline(s: str) -> bool:\n",
    "    sl = s.lower()\n",
    "    return (len(s) <= 180) and (sl.startswith(\"by \") or \"updated\" in sl or \"‚Äî\" in s or \"‚Äì\" in s)\n",
    "\n",
    "def _paragraph_blocks(lines: List[str]) -> List[str]:\n",
    "    paras, cur = [], []\n",
    "    for ln in lines:\n",
    "        if re.match(r\"^(\\*|-|‚Ä¢|\\d+\\.)\\s+\", ln):   # treat bullets as their own para\n",
    "            if cur: paras.append(\" \".join(cur)); cur = []\n",
    "            paras.append(ln)\n",
    "        else:\n",
    "            cur.append(ln)\n",
    "    if cur: paras.append(\" \".join(cur))\n",
    "    return paras\n",
    "\n",
    "def _split_sentences_news(para: str) -> List[str]:\n",
    "    marked = re.sub(r'([.!?][\"‚Äù\\']?)\\s+(?=[A-Z0-9‚Äú\"(\\[])', r'\\1<SPLIT>', para.strip())\n",
    "    parts = [p.strip() for p in marked.split('<SPLIT>') if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def _count_tokens(s: str) -> int:\n",
    "    return len(tok.encode(s, add_special_tokens=False))\n",
    "\n",
    "def chunk_news_smart(\n",
    "    raw_text: str,\n",
    "    target_tokens: int = 320,\n",
    "    overlap_tokens: int = 48,\n",
    "    *,\n",
    "    debug: bool = False,\n",
    "    debug_n: int = 5\n",
    ") -> List[str]:\n",
    "    lines = _clean_lines(raw_text)\n",
    "    if not lines:\n",
    "        return []\n",
    "\n",
    "    headline = lines[0] if _looks_headline(lines[0]) else None\n",
    "    i = 1 if headline else 0\n",
    "    dek = lines[i] if i < len(lines) and _is_dek_or_byline(lines[i]) else None\n",
    "    i = i + 1 if dek else i\n",
    "\n",
    "    body_lines = lines[i:]\n",
    "    paras = _paragraph_blocks(body_lines)\n",
    "\n",
    "    prefix = \" ‚Äî \".join([x for x in [headline, dek] if x]) if (headline or dek) else None\n",
    "    prefix_tok = _count_tokens(prefix) if prefix else 0\n",
    "\n",
    "    chunks: List[str] = []\n",
    "    current: List[str] = []\n",
    "    current_tok = 0\n",
    "\n",
    "    def flush_chunk():\n",
    "        nonlocal current, current_tok, prefix\n",
    "        if not current:\n",
    "            return\n",
    "        text = \" \".join(current)\n",
    "        if prefix and not chunks:\n",
    "            text = f\"{prefix} ‚Äî {text}\"\n",
    "        chunks.append(text)\n",
    "        back, btok = [], 0\n",
    "        for s in reversed(current):\n",
    "            t = _count_tokens(s)\n",
    "            if btok + t > overlap_tokens:\n",
    "                break\n",
    "            back.insert(0, s)\n",
    "            btok += t\n",
    "        current = back\n",
    "        current_tok = sum(_count_tokens(s) for s in current)\n",
    "        prefix = None\n",
    "\n",
    "    for para in paras:\n",
    "        sents = _split_sentences_news(para)\n",
    "        for s in sents:\n",
    "            tok_count = _count_tokens(s)\n",
    "            if tok_count > target_tokens:\n",
    "                parts = re.split(r'([,;:]\\s+)', s)\n",
    "                buf = \"\"\n",
    "                for p in parts:\n",
    "                    if _count_tokens(buf + p) > target_tokens and buf:\n",
    "                        current.append(buf.strip())\n",
    "                        flush_chunk()\n",
    "                        buf = \"\"\n",
    "                    buf += p\n",
    "                if buf.strip():\n",
    "                    current.append(buf.strip()); current_tok += _count_tokens(buf.strip())\n",
    "                continue\n",
    "\n",
    "            budget = target_tokens - (0 if chunks else prefix_tok)\n",
    "            if current_tok + tok_count <= budget or not current:\n",
    "                current.append(s); current_tok += tok_count\n",
    "            else:\n",
    "                flush_chunk()\n",
    "                current.append(s); current_tok = tok_count\n",
    "\n",
    "    flush_chunk()\n",
    "\n",
    "    if debug:\n",
    "        print(f\"üß© Built {len(chunks)} chunks (target={target_tokens}, overlap={overlap_tokens})\")\n",
    "        for i, ch in enumerate(chunks[:debug_n]):\n",
    "            t = _count_tokens(ch)\n",
    "            preview = (ch[:160] + \"‚Ä¶\") if len(ch) > 160 else ch\n",
    "            print(f\"  ‚Ä¢ chunk[{i}] tokens={t}: {preview}\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def upsert_rows(rows: List[Dict]):\n",
    "    for r in rows:\n",
    "        if len(r[\"embedding\"]) != 4000:\n",
    "            raise ValueError(\"Each embedding must be length 4000 for halfvec(4000).\")\n",
    "    resp = (\n",
    "        supabase.table(\"case_chunks\")\n",
    "        .upsert(\n",
    "            rows,\n",
    "            on_conflict=\"doc_id,chunk_id\",   # ‚Üê important\n",
    "            ignore_duplicates=False,\n",
    "            returning=\"minimal\"              # smaller response\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "    if getattr(resp, \"error\", None):\n",
    "        raise RuntimeError(resp.error)\n",
    "    print(f\"‚úÖ Upserted {len(rows)} rows into Supabase.\")\n",
    "    return resp\n",
    "\n",
    "\n",
    "# ‚òÖ NEW: if CSV gives us a stable id, use it; else fall back to link+hash(text)\n",
    "def _doc_id_from_csv(row, *, id_col=\"id\", link_col=\"link\", text_col=\"full_text\", row_idx=0) -> str:\n",
    "    csv_id = row.get(id_col)\n",
    "    if csv_id is not None and str(csv_id).strip():\n",
    "        return str(csv_id).strip()\n",
    "\n",
    "    # fallback (previous logic)\n",
    "    link = row.get(link_col)\n",
    "    text = str(row.get(text_col) or \"\")\n",
    "    base = str(link).strip() if link and str(link).strip() else None\n",
    "    h = hashlib.sha1((text or f\"row-{row_idx}\").encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return f\"{base or 'hash'}#h{h}\"\n",
    "\n",
    "\n",
    "def ingest_one_doc(full_text: str, law=None, company=None, link=None, doc_id=None):\n",
    "    if not doc_id:\n",
    "        # Last-resort fallback only if someone calls this directly without a doc_id.\n",
    "        h = hashlib.sha1((full_text or '').encode('utf-8')).hexdigest()[:10]\n",
    "        doc_id = f\"hash-{h}\"\n",
    "    print(f\"\\nüöÄ Ingesting one doc: {doc_id}\")\n",
    "\n",
    "    chunks = chunk_news_smart(full_text, target_tokens=320, overlap_tokens=48)\n",
    "    print(f\"Split into {len(chunks)} chunks.\")\n",
    "    if not chunks:\n",
    "        print(\"‚ö†Ô∏è No content after cleaning; skipping.\")\n",
    "        return\n",
    "\n",
    "    embs = embed_texts(chunks)  # [n, 4000]\n",
    "    rows = []\n",
    "    for ci, (chunk_text_val, vec) in enumerate(zip(chunks, embs)):\n",
    "        rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": ci,\n",
    "            \"text\": chunk_text_val,\n",
    "            \"law\": law,\n",
    "            \"company\": company,\n",
    "            \"link\": link,\n",
    "            \"embedding\": vec.astype(float).tolist(),\n",
    "        })\n",
    "    upsert_rows(rows)\n",
    "\n",
    "\n",
    "# ‚òÖ NEW: CSV ingestion wrapper\n",
    "def ingest_csv(\n",
    "    csv_path: str,\n",
    "    text_col: str = \"full_text\",\n",
    "    law_col: str = \"law\",\n",
    "    company_col: str = \"company\",\n",
    "    link_col: str = \"link\",\n",
    "    id_col: str = \"id\",\n",
    "):\n",
    "    print(f\"\\nüìÑ Loading CSV: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    cols_lower = {c.lower() for c in df.columns}\n",
    "    if text_col.lower() not in cols_lower:\n",
    "        raise ValueError(f\"CSV must have a '{text_col}' column. Found: {sorted(df.columns)}\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        text = str(row.get(text_col) or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        law = row.get(law_col) if law_col in df.columns else None\n",
    "        company = row.get(company_col) if company_col in df.columns else None\n",
    "        link = row.get(link_col) if link_col in df.columns else None\n",
    "\n",
    "        doc_id = _doc_id_from_csv(row, id_col=id_col, link_col=link_col, text_col=text_col, row_idx=idx)\n",
    "\n",
    "        ingest_one_doc(text, law=law, company=company, link=link, doc_id=doc_id)\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"‚Ä¶ processed {idx+1}/{len(df)} rows\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ‚òÖ NEW: run over your CSV (defaults to tech_law_violations.csv)\n",
    "    CSV_PATH = os.environ.get(\"CSV_PATH\", \"tech_law_violations.csv\")\n",
    "    ingest_csv(CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e77c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
