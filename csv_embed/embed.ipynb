{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854cef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Torch device available: cuda, CUDA count=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b0f213617240b29eccc454539ed111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded on device(s): {device(type='cuda', index=0), device(type='meta')}\n",
      "\n",
      "üìÑ Loading CSV: tech_law_violations.csv\n",
      "\n",
      "üöÄ Ingesting one doc: https://www.ftc.gov/news-events/press-releases/2019/07/ftc-imposes-5-billion-penalty-sweeping-new-privacy-restrictions-facebook\n",
      "Split into 2 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9879\n",
      "‚û°Ô∏è Embedded batch 2 on cuda:0 | shape=(1, 4000) | norm‚âà0.9924\n",
      "‚úÖ Upserted 2 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://www.ftc.gov/news-events/news/press-releases/2022/05/ftc-justice-department-order-twitter-pay-150-million-violating-2011-ftc-order-misrepresenting\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9916\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://www.ftc.gov/news-events/press-releases/2019/09/google-youtube-pay-record-170-million-penalty-alleged-violations-coppa\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9933\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://www.ftc.gov/news-events/blogs/business-blog/2019/02/largest-ftc-coppa-settlement-requires-musically-change-its-tune\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9951\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://www.ftc.gov/news-events/press-releases/2019/07/ftc-sends-settlement-agreements-cambridge-analytica-cofounder-kogan-former-ceo-alexander-nix-public\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9917\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://www.ftc.gov/news-events/press-releases/2014/05/snapchat-settles-ftc-charges-promised-dissapearing-messages-deceived-users\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9889\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://www.ftc.gov/news-events/press-releases/2020/11/ftc-approves-final-settlement-prohibiting-zoom-securely-lying-consumers-about-encryption\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9881\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://ag.ny.gov/press-release/2025/mar/ag-james-announces-settlement-saturn-technologies-protect-privacy-students\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9911\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://www.texasattorneygeneral.gov/news/releases/ag-paxton-announces-historic-settlement-recovering-14-billion-texas-fight-biometric-privacy\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9893\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://www.justice.gov/opa/pr/justice-department-secures-groundbreaking-agreement-meta-platforms-remedy-acts-discrimination\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9891\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "‚Ä¶ processed 10/51 rows\n",
      "\n",
      "üöÄ Ingesting one doc: https://ico.org.uk/about-the-ico/media-centre/news-and-blogs/2023/04/ico-fines-tiktok-12-7m-for-misusing-children-s-data/\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9869\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://edpb.europa.eu/news/news/2023/edpb-binding-decision-tiktok-inquiry-regarding-unfair-design-practices_en\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9899\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://edpb.europa.eu/news/news/2022/edpb-adopts-binding-decision-ordering-irish-dpc-to-adjust-its-approach-facebook-data_en\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9900\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://edpb.europa.eu/news/news/2022/binding-decision-instagram-inquiry-edpb-requires-irish-dpc-reassess-fine-second-largest_en\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9918\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://oag.ca.gov/news/press-releases/attorney-general-bonta-secures-500000-penalty-against-tilting-point-media\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9915\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://oag.ca.gov/news/press-releases/attorney-general-bonta-secures-settlement-against-doordash-violating-privacy-laws\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9910\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://oag.ca.gov/news/press-releases/attorney-general-bonta-sues-tiktok-exploitation-young-users\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9856\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://ag.ny.gov/press-release/2023/oct/nys-ag-james-leads-bipartisan-coalition-attorneys-general-suing-meta-youth-mental-health\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9877\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://law.alaska.gov/news/latest/2023/100523-blackbaud.html\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9884\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://noyb.eu/en/cnils-decision-criteo-fined-40-million-euros-after-noyb-complaint\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9887\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "‚Ä¶ processed 20/51 rows\n",
      "\n",
      "üöÄ Ingesting one doc: https://noyb.eu/en/spotify-fined-5-million-euro-after-noyb-complaint\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9858\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://noyb.eu/en/eu-us-data-transfers-edpb-fines-meta-record-eu-privacy-case\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9952\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://noyb.eu/en/gdpr-complaints-against-google-facebook-instagram-and-whatsapp-forced-consent\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9918\n",
      "‚úÖ Upserted 1 rows into Supabase.\n",
      "\n",
      "üöÄ Ingesting one doc: https://noyb.eu/en/gdpr-complaints-against-google-facebook-instagram-and-whatsapp-forced-consent\n",
      "Split into 1 chunks.\n",
      "‚û°Ô∏è Embedded batch 1 on cuda:0 | shape=(1, 4000) | norm‚âà0.9923\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "{'message': 'duplicate key value violates unique constraint \"case_chunks_doc_id_chunk_id_key\"', 'code': '23505', 'hint': None, 'details': 'Key (doc_id, chunk_id)=(https://noyb.eu/en/gdpr-complaints-against-google-facebook-instagram-and-whatsapp-forced-consent, 0) already exists.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 254\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# ‚òÖ NEW: run over your CSV (defaults to tech_law_violations.csv)\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     CSV_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtech_law_violations.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 254\u001b[0m     \u001b[43mingest_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 247\u001b[0m, in \u001b[0;36mingest_csv\u001b[0;34m(csv_path, text_col, law_col, company_col, link_col)\u001b[0m\n\u001b[1;32m    245\u001b[0m link \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mget(link_col) \u001b[38;5;28;01mif\u001b[39;00m link_col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m doc_id \u001b[38;5;241m=\u001b[39m _stable_doc_id(link, text, idx)\n\u001b[0;32m--> 247\u001b[0m \u001b[43mingest_one_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlaw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompany\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompany\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚Ä¶ processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 223\u001b[0m, in \u001b[0;36mingest_one_doc\u001b[0;34m(full_text, law, company, link, doc_id)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ci, (chunk_text_val, vec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(chunks, embs)):\n\u001b[1;32m    214\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc_id,\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: ci,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: vec\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    222\u001b[0m     })\n\u001b[0;32m--> 223\u001b[0m \u001b[43mupsert_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 190\u001b[0m, in \u001b[0;36mupsert_rows\u001b[0;34m(rows)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4000\u001b[39m:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach embedding must be length 4000 for halfvec(4000).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 190\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43msupabase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcase_chunks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(resp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(resp\u001b[38;5;241m.\u001b[39merror)\n",
      "File \u001b[0;32m~/AC-Acai/venv/lib/python3.12/site-packages/postgrest/_sync/request_builder.py:78\u001b[0m, in \u001b[0;36mSyncQueryRequestBuilder.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         json_obj \u001b[38;5;241m=\u001b[39m model_validate_json(APIErrorFromJSON, r\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m---> 78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m APIError(\u001b[38;5;28mdict\u001b[39m(json_obj))\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIError(generate_default_error_message(r))\n",
      "\u001b[0;31mAPIError\u001b[0m: {'message': 'duplicate key value violates unique constraint \"case_chunks_doc_id_chunk_id_key\"', 'code': '23505', 'hint': None, 'details': 'Key (doc_id, chunk_id)=(https://noyb.eu/en/gdpr-complaints-against-google-facebook-instagram-and-whatsapp-forced-consent, 0) already exists.'}"
     ]
    }
   ],
   "source": [
    "# embed.py\n",
    "# pip install pandas transformers \"supabase>=2\" python-dotenv torch\n",
    "\n",
    "import os, re, json, hashlib\n",
    "from typing import List, Dict, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from supabase import create_client, Client\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---- Env ----\n",
    "load_dotenv(\"../secrets/.env.dev\")\n",
    "SUPABASE_URL = os.environ[\"SUPABASE_URL\"]\n",
    "SUPABASE_KEY = os.environ[\"SUPABASE_KEY\"]\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# ---- Model ----\n",
    "MODEL_ID = \"Qwen/Qwen3-Embedding-8B\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üîß Torch device available: {DEVICE}, CUDA count={torch.cuda.device_count()}\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n",
    "mdl = AutoModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.bfloat16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",        # will use GPU if possible\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "mdl.eval()\n",
    "print(f\"‚úÖ Model loaded on device(s): {set(p.device for p in mdl.parameters())}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def _last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    seq_lens = attention_mask.sum(dim=1) - 1\n",
    "    bsz = last_hidden_states.shape[0]\n",
    "    return last_hidden_states[torch.arange(bsz, device=last_hidden_states.device), seq_lens]\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_texts(texts: List[str], batch_size=1, max_length=1024) -> np.ndarray:\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        toks = tok(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        dev = next(iter(mdl.state_dict().values())).device\n",
    "        toks = {k: v.to(dev) for k, v in toks.items()}\n",
    "        out = mdl(**toks)\n",
    "        pooled = _last_token_pool(out.last_hidden_state, toks[\"attention_mask\"])\n",
    "        pooled = F.normalize(pooled, p=2, dim=1)\n",
    "        pooled = pooled[:, :4000].to(torch.float32)  # keep 4000 for halfvec(4000)\n",
    "        arr = pooled.cpu().numpy()\n",
    "        norms = np.linalg.norm(arr, axis=1)\n",
    "        print(f\"‚û°Ô∏è Embedded batch {i//batch_size+1} on {dev} | shape={arr.shape} | norm‚âà{norms.mean():.4f}\")\n",
    "        vecs.append(arr)\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "# =========================\n",
    "# NEWS-AWARE CHUNKER\n",
    "# =========================\n",
    "BOILERPLATE_PATTERNS = [\n",
    "    r\"^Advertisement$\", r\"^Ads?$\", r\"^Subscribe now.*$\", r\"^Sign up for.*newsletter.*$\",\n",
    "    r\"^Read more:.*$\", r\"^Editor‚Äôs note:.*$\", r\"^Correction:.*$\", r\"^¬©.*$\"\n",
    "]\n",
    "BOILERPLATE_RE = re.compile(\"|\".join(BOILERPLATE_PATTERNS), flags=re.IGNORECASE)\n",
    "\n",
    "def _clean_lines(text: str) -> List[str]:\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\")]\n",
    "    return [ln for ln in lines if ln and not BOILERPLATE_RE.match(ln)]\n",
    "\n",
    "def _looks_headline(s: str) -> bool:\n",
    "    return len(s) <= 140 and not s.endswith(\".\") and (\n",
    "        s.istitle() or s.isupper() or re.match(r\"^[A-Z0-9][A-Za-z0-9,:‚Äô'‚Äú‚Äù\\-‚Äì ]+$\", s)\n",
    "    )\n",
    "\n",
    "def _is_dek_or_byline(s: str) -> bool:\n",
    "    sl = s.lower()\n",
    "    return (len(s) <= 180) and (sl.startswith(\"by \") or \"updated\" in sl or \"‚Äî\" in s or \"‚Äì\" in s)\n",
    "\n",
    "def _paragraph_blocks(lines: List[str]) -> List[str]:\n",
    "    paras, cur = [], []\n",
    "    for ln in lines:\n",
    "        if re.match(r\"^(\\*|-|‚Ä¢|\\d+\\.)\\s+\", ln):   # treat bullets as their own para\n",
    "            if cur: paras.append(\" \".join(cur)); cur = []\n",
    "            paras.append(ln)\n",
    "        else:\n",
    "            cur.append(ln)\n",
    "    if cur: paras.append(\" \".join(cur))\n",
    "    return paras\n",
    "\n",
    "def _split_sentences_news(para: str) -> List[str]:\n",
    "    marked = re.sub(r'([.!?][\"‚Äù\\']?)\\s+(?=[A-Z0-9‚Äú\"(\\[])', r'\\1<SPLIT>', para.strip())\n",
    "    parts = [p.strip() for p in marked.split('<SPLIT>') if p.strip()]\n",
    "    return parts\n",
    "\n",
    "def _count_tokens(s: str) -> int:\n",
    "    return len(tok.encode(s, add_special_tokens=False))\n",
    "\n",
    "def chunk_news_smart(\n",
    "    raw_text: str,\n",
    "    target_tokens: int = 320,\n",
    "    overlap_tokens: int = 48,\n",
    "    *,\n",
    "    debug: bool = False,\n",
    "    debug_n: int = 5\n",
    ") -> List[str]:\n",
    "    lines = _clean_lines(raw_text)\n",
    "    if not lines:\n",
    "        return []\n",
    "\n",
    "    headline = lines[0] if _looks_headline(lines[0]) else None\n",
    "    i = 1 if headline else 0\n",
    "    dek = lines[i] if i < len(lines) and _is_dek_or_byline(lines[i]) else None\n",
    "    i = i + 1 if dek else i\n",
    "\n",
    "    body_lines = lines[i:]\n",
    "    paras = _paragraph_blocks(body_lines)\n",
    "\n",
    "    prefix = \" ‚Äî \".join([x for x in [headline, dek] if x]) if (headline or dek) else None\n",
    "    prefix_tok = _count_tokens(prefix) if prefix else 0\n",
    "\n",
    "    chunks: List[str] = []\n",
    "    current: List[str] = []\n",
    "    current_tok = 0\n",
    "\n",
    "    def flush_chunk():\n",
    "        nonlocal current, current_tok, prefix\n",
    "        if not current:\n",
    "            return\n",
    "        text = \" \".join(current)\n",
    "        if prefix and not chunks:\n",
    "            text = f\"{prefix} ‚Äî {text}\"\n",
    "        chunks.append(text)\n",
    "        back, btok = [], 0\n",
    "        for s in reversed(current):\n",
    "            t = _count_tokens(s)\n",
    "            if btok + t > overlap_tokens:\n",
    "                break\n",
    "            back.insert(0, s)\n",
    "            btok += t\n",
    "        current = back\n",
    "        current_tok = sum(_count_tokens(s) for s in current)\n",
    "        prefix = None\n",
    "\n",
    "    for para in paras:\n",
    "        sents = _split_sentences_news(para)\n",
    "        for s in sents:\n",
    "            tok_count = _count_tokens(s)\n",
    "            if tok_count > target_tokens:\n",
    "                parts = re.split(r'([,;:]\\s+)', s)\n",
    "                buf = \"\"\n",
    "                for p in parts:\n",
    "                    if _count_tokens(buf + p) > target_tokens and buf:\n",
    "                        current.append(buf.strip())\n",
    "                        flush_chunk()\n",
    "                        buf = \"\"\n",
    "                    buf += p\n",
    "                if buf.strip():\n",
    "                    current.append(buf.strip()); current_tok += _count_tokens(buf.strip())\n",
    "                continue\n",
    "\n",
    "            budget = target_tokens - (0 if chunks else prefix_tok)\n",
    "            if current_tok + tok_count <= budget or not current:\n",
    "                current.append(s); current_tok += tok_count\n",
    "            else:\n",
    "                flush_chunk()\n",
    "                current.append(s); current_tok = tok_count\n",
    "\n",
    "    flush_chunk()\n",
    "\n",
    "    if debug:\n",
    "        print(f\"üß© Built {len(chunks)} chunks (target={target_tokens}, overlap={overlap_tokens})\")\n",
    "        for i, ch in enumerate(chunks[:debug_n]):\n",
    "            t = _count_tokens(ch)\n",
    "            preview = (ch[:160] + \"‚Ä¶\") if len(ch) > 160 else ch\n",
    "            print(f\"  ‚Ä¢ chunk[{i}] tokens={t}: {preview}\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def upsert_rows(rows: List[Dict]):\n",
    "    for r in rows:\n",
    "        if len(r[\"embedding\"]) != 4000:\n",
    "            raise ValueError(\"Each embedding must be length 4000 for halfvec(4000).\")\n",
    "    resp = (\n",
    "        supabase.table(\"case_chunks\")\n",
    "        .upsert(\n",
    "            rows,\n",
    "            on_conflict=\"doc_id,chunk_id\",   # ‚Üê important\n",
    "            ignore_duplicates=False,\n",
    "            returning=\"minimal\"              # smaller response\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "    if getattr(resp, \"error\", None):\n",
    "        raise RuntimeError(resp.error)\n",
    "    print(f\"‚úÖ Upserted {len(rows)} rows into Supabase.\")\n",
    "    return resp\n",
    "\n",
    "\n",
    "# ‚òÖ NEW: if CSV gives us a stable id, use it; else fall back to link+hash(text)\n",
    "def _doc_id_from_csv(row, *, id_col=\"id\", link_col=\"link\", text_col=\"full_text\", row_idx=0) -> str:\n",
    "    csv_id = row.get(id_col)\n",
    "    if csv_id is not None and str(csv_id).strip():\n",
    "        return str(csv_id).strip()\n",
    "\n",
    "    # fallback (previous logic)\n",
    "    link = row.get(link_col)\n",
    "    text = str(row.get(text_col) or \"\")\n",
    "    base = str(link).strip() if link and str(link).strip() else None\n",
    "    h = hashlib.sha1((text or f\"row-{row_idx}\").encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return f\"{base or 'hash'}#h{h}\"\n",
    "\n",
    "\n",
    "def ingest_one_doc(full_text: str, law=None, company=None, link=None, doc_id=None):\n",
    "    if not doc_id:\n",
    "        # Last-resort fallback only if someone calls this directly without a doc_id.\n",
    "        h = hashlib.sha1((full_text or '').encode('utf-8')).hexdigest()[:10]\n",
    "        doc_id = f\"hash-{h}\"\n",
    "    print(f\"\\nüöÄ Ingesting one doc: {doc_id}\")\n",
    "\n",
    "    chunks = chunk_news_smart(full_text, target_tokens=320, overlap_tokens=48)\n",
    "    print(f\"Split into {len(chunks)} chunks.\")\n",
    "    if not chunks:\n",
    "        print(\"‚ö†Ô∏è No content after cleaning; skipping.\")\n",
    "        return\n",
    "\n",
    "    embs = embed_texts(chunks)  # [n, 4000]\n",
    "    rows = []\n",
    "    for ci, (chunk_text_val, vec) in enumerate(zip(chunks, embs)):\n",
    "        rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": ci,\n",
    "            \"text\": chunk_text_val,\n",
    "            \"law\": law,\n",
    "            \"company\": company,\n",
    "            \"link\": link,\n",
    "            \"embedding\": vec.astype(float).tolist(),\n",
    "        })\n",
    "    upsert_rows(rows)\n",
    "\n",
    "\n",
    "# ‚òÖ NEW: CSV ingestion wrapper\n",
    "def ingest_csv(\n",
    "    csv_path: str,\n",
    "    text_col: str = \"full_text\",\n",
    "    law_col: str = \"law\",\n",
    "    company_col: str = \"company\",\n",
    "    link_col: str = \"link\",\n",
    "    id_col: str = \"id\",\n",
    "):\n",
    "    print(f\"\\nüìÑ Loading CSV: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    cols_lower = {c.lower() for c in df.columns}\n",
    "    if text_col.lower() not in cols_lower:\n",
    "        raise ValueError(f\"CSV must have a '{text_col}' column. Found: {sorted(df.columns)}\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        text = str(row.get(text_col) or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        law = row.get(law_col) if law_col in df.columns else None\n",
    "        company = row.get(company_col) if company_col in df.columns else None\n",
    "        link = row.get(link_col) if link_col in df.columns else None\n",
    "\n",
    "        doc_id = _doc_id_from_csv(row, id_col=id_col, link_col=link_col, text_col=text_col, row_idx=idx)\n",
    "\n",
    "        ingest_one_doc(text, law=law, company=company, link=link, doc_id=doc_id)\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"‚Ä¶ processed {idx+1}/{len(df)} rows\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ‚òÖ NEW: run over your CSV (defaults to tech_law_violations.csv)\n",
    "    CSV_PATH = os.environ.get(\"CSV_PATH\", \"tech_law_violations.csv\")\n",
    "    ingest_csv(CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e77c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
